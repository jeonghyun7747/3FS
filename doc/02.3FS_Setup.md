# 3FS Setup Guide

This section provides a manual deployment guide for setting up a six-node cluster with the cluster ID `stage`.

## Installation prerequisites

### Hardware specifications

| Node     | OS           | IP           | Memory | SSD       | RDMA     |
| -------- | ------------ | ------------ | ------ | --------- | -------- |
| meta     | Ubuntu 22.04 | 192.168.0.21 | 4GB    | -         | SoftRoCE |
| storage1 | Ubuntu 22.04 | 192.168.0.22 | 4GB    | 100GB × 2 | SofRoCE  |
| storage2 | Ubuntu 22.04 | 192.168.0.23 | 4GB    | 100GB × 2 | SofRoCE  |
| storage3 | Ubuntu 22.04 | 192.168.0.24 | 4GB    | 100GB × 2 | SoftRoCE |

> **RDMA Configuration**
>
> 1. Assign IP addresses to RDMA NICs. Multiple RDMA NICs (InfiniBand or RoCE) are supported on each node.
> 2. Check RDMA connectivity between nodes using `ib_write_bw`.

### Third-party dependencies

In production environment, it is recommended to install FoundationDB and ClickHouse on dedicated nodes.

| Service                                                                  | Node |
| ------------------------------------------------------------------------ | ---- |
| [ClickHouse](https://clickhouse.com/docs/install)                        | meta |
| [FoundationDB](https://apple.github.io/foundationdb/administration.html) | meta |

#### 1. ClickHouse

[https://clickhouse.com/docs/install](https://clickhouse.com/docs/install)

```sh
$ curl https://clickhouse.com/ | sh

Successfully downloaded the ClickHouse binary, you can run it as:
$ ./clickhouse

You can also install it:
$ sudo ./clickhouse install

Start clickhouse-server with:
$ sudo clickhouse start

Start clickhouse-client with:
$ clickhouse-client --password
v51 :) show users;
v51 :) show databases;
v51 :) use 3fs
v51 :) show tables;
```

- port 수정할 경우

```sh
$ sudo  vi  /etc/clickhouse-server/config.xml
$ <tcp_port>9500</tcp_port>

$ clickhouse-client --password --port 9500
```

- reset

```sh
$ sudo  systemctl disable  clickhouse
Removed /etc/systemd/system/multi-user.target.wants/clickhouse.service.
$ sudo ./clickhouse stop
$ sudo rm /var/lib/clickhouse
$ sudo rm /var/log/clickhouse-server
$ sudo rm /etc/clickhouse-server/users.xml
$ sudo rm /etc/security/limits.d/clickhouse.conf
$ sudo rm /etc/clickhouse-server/users.xml
$ sudo rm /usr/lib/systemd/system/clickhouse.service
```

- systemd 설정

```sh
$ sudo vi /etc/systemd/system/clickhouse.service

[Unit]
Description=Clickhouse
After=network-online.target
Wants=network-online.target

[Service]
Type=oneshot
ExecStart=/usr/bin/clickhouse  start
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
```

```sh
$ sudo chmod  644 /usr/lib/systemd/system/clickhouse.service
$ sudo systemctl daemon-reload
$ sudo systemctl start clickhouse
$ sudo systemctl enable clickhouse
```

#### 2. FoundationDB

> 1. Ensure that the version of FoundationDB client matches the server version, or copy the corresponding version of `libfdb_c.so` to maintain compatibility.
> 2. Find the `fdb.cluster` file and `libfdb_c.so` at `/etc/foundationdb/fdb.cluster`, `/usr/lib/libfdb_c.so` on nodes with FoundationDB installed.

- download & install

```sh
$ wget --no-check-certificate  https://github.com/apple/foundationdb/releases/download/7.1.67/foundationdb-clients_7.1.67-1_amd64.deb
$ wget --no-check-certificate  https://github.com/apple/foundationdb/releases/download/7.1.67/foundationdb-server_7.1.67-1_amd64.deb
$ sudo dpkg -i foundationdb-clients_7.1.67-1_amd64.deb  foundationdb-server_7.1.67-1_amd64.deb
```

#### 3. 3FS 라이브러리 설치

```sh
$ sudo  apt install cmake libuv1-dev liblz4-dev liblzma-dev libdouble-conversion-dev libdwarf-dev libunwind-dev \
  libaio-dev libgflags-dev libgoogle-glog-dev libgtest-dev libgmock-dev clang-format-14 clang-14 clang-tidy-14 lld-14 \
  libgoogle-perftools-dev google-perftools libssl-dev gcc-12 g++-12 libboost-all-dev
```

#### 4. check port and process

```sh
$ sudo ss -lntp
State    Recv-Q   Send-Q   Local Address:Port  Peer Address:Port  Process
LISTEN   0        4096                 *:8123             *:*      users:(("clickhouse-serv",pid=6121,fd=35))
LISTEN   0        4096                 *:9009             *:*      users:(("clickhouse-serv",pid=6121,fd=28))
LISTEN   0        4096                 *:9004             *:*      users:(("clickhouse-serv",pid=6121,fd=37))
LISTEN   0        4096                 *:9005             *:*      users:(("clickhouse-serv",pid=6121,fd=29))
LISTEN   0        4096                 *:9500             *:*      users:(("clickhouse-serv",pid=6121,fd=36))
LISTEN   0        128          127.0.0.1:4500       0.0.0.0:*      users:(("fdbserver",pid=1652,fd=18))

$ sudo  ps -efl | egrep  'click|found'
1 S root        1650       1  0  80   0 -  1188 do_sel 12:12 ?        00:00:00 /usr/lib/foundationdb/fdbmonitor --conffile /etc/foundationdb/foundationdb.conf --lockfile /var/run/fdbmonitor.pid --daemonize
4 S foundat+    1651    1650  0  80   0 - 44609 ep_pol 12:12 ?        00:00:05 /usr/lib/foundationdb/backup_agent/backup_agent --cluster-file /etc/foundationdb/fdb.cluster --logdir /var/log/foundationdb
4 S foundat+    1652    1650  1  80   0 - 53698 ep_pol 12:12 ?        00:00:18 /usr/sbin/fdbserver --cluster-file /etc/foundationdb/fdb.cluster --datadir /var/lib/foundationdb/data/4500 --listen-address public --logdir /var/log/foundationdb --public-address auto:4500
5 S clickho+    6121    6120 11  80   0 - 2054954 futex_ 12:29 ?      00:00:34 /usr/bin/clickhouse-server --config-file /etc/clickhouse-server/config.xml --pid-file /var/run/clickhouse-server/clickhouse-server.pid --daemon
```

- rdma check

```sh
$ which ibdev2netdev
/usr/sbin/ibdev2netdev

root@v51:~# ibv_devinfo
hca_id:	rxe0
	transport:			InfiniBand (0)
	fw_ver:				0.0.0
	node_guid:			0a00:27ff:fe37:d80e
	sys_image_guid:			0a00:27ff:fe37:d80e
	vendor_id:			0xffffff
	vendor_part_id:			0
	hw_ver:				0x0
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		1024 (3)
			sm_lid:			0
			port_lid:		0
			port_lmc:		0x00
			link_layer:		Ethernet
```

---

## Step 0: Build 3FS

Follow the [instructions](../README.md#build-3fs) to build 3FS. Binaries can be found in `build/bin`.

### Services and clients

The following steps show how to install 3FS services in `/opt/3fs/bin` and the config files in `/opt/3fs/etc`.

| Service   | Binary                 | Config files                                                                                                                                                                             | NodeID      | Node                                                             |
| --------- | ---------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | ---------------------------------------------------------------- |
| monitor   | monitor_collector_main | [monitor_collector_main.toml](../configs/monitor_collector_main.toml)                                                                                                                    | -           | meta                                                             |
| admin_cli | admin_cli              | [admin_cli.toml](../configs/admin_cli.toml)<br>fdb.cluster                                                                                                                               | -           | meta<br>storage1<br>storage2<br>storage3<br>storage4<br>storage5 |
| mgmtd     | mgmtd_main             | [mgmtd_main_launcher.toml](../configs/mgmtd_main_launcher.toml)<br>[mgmtd_main.toml](../configs/mgmtd_main.toml)<br>[mgmtd_main_app.toml](../configs/mgmtd_main_app.toml)<br>fdb.cluster | 1           | meta                                                             |
| meta      | meta_main              | [meta_main_launcher.toml](../configs/meta_main_launcher.toml)<br>[meta_main.toml](../configs/meta_main.toml)<br>[meta_main_app.toml](../configs/meta_main_app.toml)<br>fdb.cluster       | 100         | meta                                                             |
| storage   | storage_main           | [storage_main_launcher.toml](../configs/storage_main_launcher.toml)<br>[storage_main.toml](../configs/storage_main.toml)<br>[storage_main_app.toml](../configs/storage_main_app.toml)    | 10001~10005 | storage1<br>storage2<br>storage3<br>storage4<br>storage5         |
| client    | hf3fs_fuse_main        | [hf3fs_fuse_main_launcher.toml](../configs/hf3fs_fuse_main_launcher.toml)<br>[hf3fs_fuse_main.toml](../configs/hf3fs_fuse_main.toml)                                                     | -           | meta                                                             |

```sh
$ cat /etc/hosts
192.168.0.51 meta
192.168.0.52 storage1
192.168.0.53 storage2
192.168.0.54 storage3
192.168.0.55 client
192.168.0.100 happy
```

---

## Step 1: Create ClickHouse tables for metrics

Import the SQL file into ClickHouse:

```sh
$ git clone https://github.com/deepseek-ai/3fs
$ cd 3fs
$ git submodule update --init --recursive
$ ./patches/apply.sh

$ clickhouse-client --password  < ~/3fs/deploy/sql/3fs-monitor.sql
v21 :) show users;
v21 :) show databases;
v21 :) use 3fs
v21 :) show tables;

$ sudo ss -lntp
```

---

## Step 2: Monitor service

Install `monitor_collector` service on the **meta** node.

1. Copy `monitor_collector_main` to `/opt/3fs/bin` and config files to `/opt/3fs/etc`, and create log directory `/var/log/3fs`.

```sh
$ sudo mkdir -p /opt/3fs/{bin,etc}
$ sudo mkdir -p /var/log/3fs
$ sudo cp ~/3fs/build/bin/monitor_collector_main /opt/3fs/bin
$ sudo cp ~/3fs/configs/monitor_collector_main.toml /opt/3fs/etc
```

2. Update [`monitor_collector_main.toml`] to add a ClickHouse connection:

```sh
$ sudo vi /opt/3fs/etc/monitor_collector_main.toml
```

```yaml
   [server.monitor_collector.reporter]
   type = 'clickhouse'

   [server.monitor_collector.reporter.clickhouse]
   db = '3fs'
   host = '192.168.0.51'
   passwd = 'ok'
   port = '9500'
   user = 'default'
```

3. Start monitor service:

```sh
$  sudo cp  ~/3fs/deploy/systemd/monitor_collector_main.service /etc/systemd/system
$  sudo chmod  644 /etc/systemd/system/monitor_collector_main.service
$  cat  /usr/lib/systemd/system/monitor_collector_main.service

[Unit]
Description=monitor_collector_main Server
Requires=network-online.target
After=network-online.target

[Service]
ExecStart=/opt/3fs/bin/monitor_collector_main --cfg /opt/3fs/etc/monitor_collector_main.toml
Type=simple

[Install]
WantedBy=multi-user.target

$ sudo systemctl daemon-reload
$ sudo systemctl start monitor_collector_main
$ sudo systemctl enable monitor_collector_main

$ sudo  systemctl restart monitor_collector_main
$ sudo  systemctl status  monitor_collector_main
```

Note that

> - Multiple instances of monitor services can be deployed behind a virtual IP address to share the traffic.
> - Other services communicate with the monitor service over a TCP connection.

---

## Step 3: Admin client

Install `admin_cli` on **all** nodes.

1. Copy `admin_cli` to `/opt/3fs/bin` and config files to `/opt/3fs/etc`.

- meta server

```sh
$ sudo cp  /etc/foundationdb/fdb.cluster  /opt/3fs/etc/fdb.cluster
```

- all server

```sh
$ sudo  mkdir -p /opt/3fs/{bin,etc}
$ sudo scp  jhyunlee@meta:/opt/3fs/bin/admin_cli /opt/3fs/bin/admin_cli
$ sudo scp  jhyunlee@meta:/opt/3fs/etc/admin_cli.toml /opt/3fs/etc/admin_cli.toml
$ sudo scp  jhyunlee@meta:/opt/3fs/etc/fdb.cluster /opt/3fs/etc/fdb.cluster
```

2. Update [`admin_cli.toml`] to set `cluster_id` and `clusterFile`:

```sh
$ sudo vi /opt/3fs/etc/admin_cli.toml
```

```yaml
cluster_id = "stage"

[fdb]
clusterFile = '/opt/3fs/etc/fdb.cluster'
```

The full help documentation for `admin_cli` can be displayed by running the following command:

```bash
$ sudo /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml help
```

---

## Step 4: Mgmtd service

Install `mgmtd` service on **meta** node.
주의사항: 설치 할때 log를 확인하면서 진행한다

```sh
$ sudo tail  -f /var/log/3fs/mgmtd_main-err.log
$ sudo tail  -f /var/log/3fs/mgmtd_main.log
```

1. Copy `mgmtd_main` to `/opt/3fs/bin` and config files to `/opt/3fs/etc`.

```sh
$ sudo cp ~/3fs/build/bin/mgmtd_main /opt/3fs/bin
$ sudo cp ~/3fs/configs/{mgmtd_main.toml,mgmtd_main_launcher.toml,mgmtd_main_app.toml} /opt/3fs/etc
```

2. Update config files:

- Set mgmtd `node_id = 1`

```sh
$ sudo vi /opt/3fs/etc/mgmtd_main_app.toml
allow_empty_node_id = true
node_id = 1

```

- Edit [`mgmtd_main_launcher.toml`] to set the `cluster_id` and `clusterFile`:

```sh
$ sudo vi /opt/3fs/etc/mgmtd_main_launcher.toml
```

```yaml
cluster_id = "stage"

[fdb]
clusterFile = '/opt/3fs/etc/fdb.cluster'
```

- Set monitor address in [`mgmtd_main.toml`]

```sh
$ sudo vi  /opt/3fs/etc/mgmtd_main.toml
```

```yaml
   [common.monitor.reporters.monitor_collector]
   remote_ip = "192.168.0.51:10000"
```

3. Initialize the cluster:

```sh
$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml "init-cluster --mgmtd /opt/3fs/etc/mgmtd_main.toml 1 1048576 16"
Init filesystem, root directory layout: chain table ChainTableId(1), chunksize 1048576, stripesize 16

Init config for MGMTD version 1
```

The parameters of `admin_cli`:

> - `1` the chain table ID
> - `1048576` the chunk size in bytes
> - `16` the file strip size

Run `help init-cluster` for full documentation.

4. Start mgmtd service:

```bash
$  sudo cp ~/3fs/deploy/systemd/mgmtd_main.service /etc/systemd/system
$  sudo chmod  664 /etc/systemd/system/mgmtd_main.service
$  sudo cat /etc/systemd/system/mgmtd_main.service
[Unit]
Description=mgmtd_main Server
Requires=network-online.target
After=network-online.target

[Service]
LimitNOFILE=1000000
ExecStart=/opt/3fs/bin/mgmtd_main --launcher_cfg /opt/3fs/etc/mgmtd_main_launcher.toml --app-cfg /opt/3fs/etc/mgmtd_main_app.toml
Type=simple

[Install]
WantedBy=multi-user.target

$  sudo /opt/3fs/bin/mgmtd_main --launcher_cfg /opt/3fs/etc/mgmtd_main_launcher.toml --app-cfg /opt/3fs/etc/mgmtd_main_app.toml
$  sudo systemctl daemon-reload
$  sudo systemctl start mgmtd_main
$  sudo systemctl enable mgmtd_main

root@v51:~# ss -lntp
State    Recv-Q Send-Q  Local Address:Port   Peer Address:Port   Process
LISTEN   0      4096     192.168.0.51:9000        0.0.0.0:*       users:(("mgmtd_main",pid=58871,fd=46))
LISTEN   0      4096     192.168.0.51:8000        0.0.0.0:*       users:(("mgmtd_main",pid=58871,fd=45))
LISTEN   0      4096     192.168.0.51:10000       0.0.0.0:*       users:(("monitor_collect",pid=1954,fd=29))
LISTEN   0      128         127.0.0.1:4500        0.0.0.0:*       users:(("fdbserver",pid=1971,fd=18))
LISTEN   0      4096                *:8123              *:*       users:(("clickhouse-serv",pid=2109,fd=54))
LISTEN   0      4096                *:9500              *:*       users:(("clickhouse-serv",pid=2109,fd=57))
LISTEN   0      4096                *:9009              *:*       users:(("clickhouse-serv",pid=2109,fd=28))
LISTEN   0      4096                *:9004              *:*       users:(("clickhouse-serv",pid=2109,fd=58))
LISTEN   0      4096                *:9005              *:*       users:(("clickhouse-serv",pid=2109,fd=46))
```

5. Run `list-nodes` command to check if the cluster has been successfully initialized:

```sh
$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' "list-nodes"

Id  Type   Status         Hostname  Pid    Tags  LastHeartbeatTime  ConfigVersion  ReleaseVersion
1   MGMTD  PRIMARY_MGMTD  v51       58871  []    N/A                1(UPTODATE)    250228-dev-1-999999-3c100b90

```

## If multiple instances of `mgmtd` services deployed, one of the `mgmtd` services is elected as the primary; others are secondaries. Automatic failover occurs when the primary fails.

## Step 5: Meta service

Install `meta` service on **meta** node.

1. Copy `meta_main` to `/opt/3fs/bin` and config files to `/opt/3fs/etc`.

```sh
$ sudo cp ~/3fs/build/bin/meta_main /opt/3fs/bin
$ sudo cp ~/3fs/configs/{meta_main_launcher.toml,meta_main.toml,meta_main_app.toml} /opt/3fs/etc
```

2. Update config files:

- Set meta `node_id = 100`

```sh
$ sudo vi /opt/3fs/etc/meta_main_app.toml
$ sudo cat  /opt/3fs/etc/meta_main_app.toml
allow_empty_node_id = true
node_id = 100
```

- Set `cluster_id`, `clusterFile` and mgmtd address in [`meta_main_launcher.toml`]

```sh
$ sudo vi  /opt/3fs/etc/meta_main_launcher.toml
```

```yaml
cluster_id = "stage"

[mgmtd_client]
mgmtd_server_addresses = ["RDMA://192.168.0.51:8000"]
```

- Set mgmtd and monitor addresses in [`meta_main.toml`]

```sh
$ sudo vi  /opt/3fs/etc/meta_main.toml
```

```yaml
   [server.mgmtd_client]
   mgmtd_server_addresses = ["RDMA://192.168.0.51:8000"]

   [common.monitor.reporters.monitor_collector]
   remote_ip = "192.168.0.51:10000"

   [server.fdb]
   clusterFile = '/opt/3fs/etc/fdb.cluster'
```

3. Config file of meta service is managed by mgmtd service. Use `admin_cli` to upload the config file to mgmtd:

- toml 파일의 설정이 변경될때 마다 다음 작업을 해줘서 버젼을 업데이트 해줘야 하다.

```sh
$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' "set-config --type META --file /opt/3fs/etc/meta_main.toml"

Succeed
ConfigVersion
```

4. Start meta service:

```bash
$ sudo  cp ~/3fs/deploy/systemd/meta_main.service /etc/systemd/system
$ sudo  chmod  644 /etc/systemd/system/meta_main.service
$ sudo  cat /etc/systemd/system/meta_main.service
[Unit]
Description=meta_main Server
Requires=network-online.target
After=network-online.target

[Service]
LimitNOFILE=1000000
ExecStart=/opt/3fs/bin/meta_main --launcher_cfg /opt/3fs/etc/meta_main_launcher.toml --app-cfg /opt/3fs/etc/meta_main_app.toml
Type=simple

[Install]
WantedBy=multi-user.target

$ sudo  /opt/3fs/bin/meta_main --launcher_cfg /opt/3fs/etc/meta_main_launcher.toml --app-cfg /opt/3fs/etc/meta_main_app.toml
$ sudo systemctl daemon-reload
$ sudo systemctl start meta_main
$ sudo systemctl enable meta_main

root@v51:~# ss -lntp
State    Recv-Q  Send-Q Local Address:Port  Peer Address:Port  Process
LISTEN   0       4096    192.168.0.51:10000      0.0.0.0:*      users:(("monitor_collect",pid=1954,fd=29))
LISTEN   0       4096    192.168.0.51:9000       0.0.0.0:*      users:(("mgmtd_main",pid=58871,fd=46))
LISTEN   0       4096    192.168.0.51:8000       0.0.0.0:*      users:(("mgmtd_main",pid=58871,fd=45))
LISTEN   0       4096    192.168.0.51:9001       0.0.0.0:*      users:(("meta_main",pid=109567,fd=54))
LISTEN   0       4096    192.168.0.51:8001       0.0.0.0:*      users:(("meta_main",pid=109567,fd=53))
LISTEN   0       128        127.0.0.1:4500       0.0.0.0:*      users:(("fdbserver",pid=1971,fd=18))
LISTEN   0       4096               *:8123             *:*      users:(("clickhouse-serv",pid=2109,fd=54))
LISTEN   0       4096               *:9500             *:*      users:(("clickhouse-serv",pid=2109,fd=57))
LISTEN   0       4096               *:9009             *:*      users:(("clickhouse-serv",pid=2109,fd=28))
LISTEN   0       4096               *:9004             *:*      users:(("clickhouse-serv",pid=2109,fd=58))
LISTEN   0       4096               *:9005             *:*      users:(("clickhouse-serv",pid=2109,fd=46))

```

5. Run `list-nodes` command to check if meta service has joined the cluster:

```sh
$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' "list-nodes"

Id   Type   Status               Hostname  Pid     Tags  LastHeartbeatTime    ConfigVersion  ReleaseVersion
1    MGMTD  PRIMARY_MGMTD        v51       58871   []    N/A                  1(UPTODATE)    250228-dev-1-999999-3c100b90
100  META   HEARTBEAT_CONNECTED  v51       123812  []    2025-03-23 13:41:57  2(UPTODATE)    250228-dev-1-999999-3c100b90

```

If multiple instances of `meta` services deployed, meta requests will be evenly distributed to all instances.

- check prcess and port

```sh
$ watch -d  -n 1 "rsh  v51 \"ps -ef | grep  3fs\" && rsh v51 \"ss -lntp\"  "
$ watch -d  -n 1 "rsh  v52 \"ps -ef | grep  3fs\" && rsh v52 \"ss -lntp\"  "
```

---

## Step 6: Storage service
* HDD 100GB disk 2개 
```sh
$ sudo lsblk 
$ sudo mkfs.xfs /dev/sdb
$ sudo mkfs.xfs /dev/sdc
$ sudo mkdir -p  /storage/data{1..2}
$ sudo mount -o noatime,nodiratime /dev/sdb /storage/data1
$ sudo mount -o noatime,nodiratime /dev/sdc /storage/data2
$ sudo mkdir -p /storage/data{1..2}/3fs
$ sudo mkdir -p /storage/data{1..2}/3fs

$ lsblk -o name,uuid,mountpoint
sdb    efa94240-2933-450c-b26f-9e64e3dd14fa /storage/data1
sdc    b90401a5-eeef-4e18-a5da-2fe0353b26b8 /storage/data2

$ sudo  blkid /dev/sdb /dev/sdc
/dev/sdb: UUID="efa94240-2933-450c-b26f-9e64e3dd14fa" BLOCK_SIZE="512" TYPE="xfs"
/dev/sdc: UUID="b90401a5-eeef-4e18-a5da-2fe0353b26b8" BLOCK_SIZE="512" TYPE="xfs"

$ sudo vi /etc/fstab 
UUID=efa94240-2933-450c-b26f-9e64e3dd14fa /storage/data1 xfs defaults,noatime,nodiratime  0 2
UUID=b90401a5-eeef-4e18-a5da-2fe0353b26b8 /storage/data2 xfs defaults,noatime,nodiratime  0 2
```

* NVME 100GB * disk 2개

```sh
$ sudo apt install nvme-cli xfsprogs
$ sudo nvme list
$ sudo mkfs.xfs /dev/nvme0n1
$ sudo mkfs.xfs /dev/nvme0n2
$ sudo mkdir -p  /storage/data{1..2}
$ sudo mount /dev/nvme0n1 /storage/data1
$ sudo mount /dev/nvme0n2 /storage/data2
$ sudo for i in {1..2};do mkfs.xfs -L data${i} /dev/nvme0n${i};mount -o noatime,nodiratime /dev/nvme0n${i} /storage/data${i};done
$ sudo mkdir -p /storage/data{1..2}/3fs
$ sudo mkdir -p /storage/data{1..2}/3fs
for i in {1..2}
do
   mkfs.xfs -L data${i} /dev/nvme0n${i}
   mount -o noatime,nodiratime /dev/nvme0n${i} /storage/data${i}
done
```




Install `storage` service on **storage** node.

1. Format the attached 16 SSDs as XFS and mount at `/storage/data{1..16}`, then create data directories `/storage/data{1..16}/3fs` and log directory `/var/log/3fs`.

```sh
$ sudo mkdir -p /storage/data{1..16}
$ sudo  mkdir -p /var/log/3fs
   for i in {1..16};do mkfs.xfs -L data${i} /dev/nvme0n${i};mount -o noatime,nodiratime -L data${i} /storage/data${i};done
$ mkdir -p /storage/data{1..16}/3fs
```

2. Increase the max number of asynchronous aio requests:

```sh
$ sudo sysctl -w fs.aio-max-nr=67108864
```

3. Copy `storage_main` to `/opt/3fs/bin` and config files to `/opt/3fs/etc`.

```sh
$   ssh v52
$   sudo  scp  jhyunlee@meta:~/3fs/build/bin/storage_main /opt/3fs/bin
$   sudo  scp  jhyunlee@meta:~/3fs/configs/{storage_main_launcher.toml,storage_main.toml,storage_main_app.toml} /opt/3fs/etc
$   sudo  scp  jhyunlee@meta:~/3fs/configs/storage_main_app.toml /opt/3fs/etc
$   sudo  scp  jhyunlee@meta:~/3fs/configs/storage_main.toml /opt/3fs/etc

```

4. Update config files:

* Set `node_id` in [`storage_main_app.toml`].
 Each storage service is assigned a unique id between `10001` and `10005`.

```sh
$ sudo vi /opt/3fs/etc/storage_main_app.toml
   allow_empty_node_id = true
   node_id = 10001
```

* Set `cluster_id` and mgmtd address in [`storage_main_launcher.toml`]

```sh
$  sudo vi /opt/3fs/etc/storage_main_launcher.toml
```

```yaml
    cluster_id = "stage"

    [mgmtd_client]
    mgmtd_server_addresses = ["RDMA://192.168.0.51:8000"]
```

* Add target paths in [`storage_main.toml`]

```sh
$  sudo vi /opt/3fs/etc/storage_main.toml
```

```yaml
   [server.mgmtd]
   mgmtd_server_addresses = ["RDMA://192.168.0.21:8000"]

   [common.monitor.reporters.monitor_collector]
   remote_ip = "192.168.0.51:10000"

   [server.targets]
   target_paths = ["/storage/data1/3fs","/storage/data2/3fs",]
```

* reduce memory usage for singel node 
https://github.com/deepseek-ai/3FS/pull/180/files

```sh
$ sudo vi /opt/3fs/etc/storage_main.toml

  [server.aio_read_worker]
  enable_io_uring = false

  [server.buffer_pool]
  big_rdmabuf_count = 16
  big_rdmabuf_size = '16MB'
  rdmabuf_count = 32
  rdmabuf_size = '4MB'
  
  [server.targets.storage_target.kv_store]
  leveldb_block_cache_size = '1GB'
  rocksdb_block_cache_size = '1GB'
```
* too many file open error
```sh
$  ulimit -n
$  ulimit -Hn 
$  ulimit -n 1000000
$  sudo -i
#  ulimit -n 1000000


$ sudo vi /etc/security/limits.conf
jhyunlee  soft nofile 20000
jhyunlee  hard nofile 1000000
root      soft nofile 20000     # <<----- sudo command에서 적용되는 값  
root      hard nofile 1000000   # <<----- sudo command에서 적용되는 값

$ sudo vi /etc/sysctl.conf
fs.file-max =1000000

$ sudo sysctl -p 

# system daemon에서 처리 방법 
[Service]
LimitNOFILE=1000000
```


5. Config file of storage service is managed by mgmtd service. Use `admin_cli` to upload the config file to mgmtd:

```sh
$ sudo /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' "set-config --type STORAGE --file /opt/3fs/etc/storage_main.toml"

Succeed
ConfigVersion  1

```

6. Start storage service:

* toml 수정이 있은 다음에는 반드시 admin_cli -cfg를 통해서 설정 변경을 반영해야 한다. 
* admin_cli를 하지 않으면 전단계의 설정 파일 가지고 애를 쓰다가 잘 안된다. 
```sh
$ sudo scp  jhyunlee@meta:~/3fs/deploy/systemd/storage_main.service /etc/systemd/system
$ sudo chmod 644  /etc/systemd/system/storage_main.service
$ sudo  cat   /etc/systemd/system/storage_main.service
[Unit]
Description=storage_main Server
Requires=network-online.target
After=network-online.target

[Service]
LimitNOFILE=1000000
LimitMEMLOCK=infinity
TimeoutStopSec=5m
ExecStart=/opt/3fs/bin/storage_main --launcher_cfg /opt/3fs/etc/storage_main_launcher.toml --app-cfg /opt/3fs/etc/storage_main_app.toml
Type=simple

[Install]
WantedBy=multi-user.target


$ sudo  /opt/3fs/bin/storage_main --launcher_cfg /opt/3fs/etc/storage_main_launcher.toml --app-cfg /opt/3fs/etc/storage_main_app.toml
$ sudo systemctl daemon-reload
$ sudo systemctl start storage_main
$ sudo systemctl enable storage_main
Created symlink /etc/systemd/system/multi-user.target.wants/storage_main.service → /etc/systemd/system/storage_main.service.

root@v52:~# ss -nltp
State           Recv-Q          Send-Q                    Local Address:Port                     Peer Address:Port          Process                                              
LISTEN          0               4096                       192.168.0.52:8000                          0.0.0.0:*              users:(("storage_main",pid=8138,fd=60))             
LISTEN          0               4096                       192.168.0.52:9000                          0.0.0.0:*              users:(("storage_main",pid=8138,fd=61))             
```


7. Run `list-nodes` command to check if storage service has joined the cluster:

```sh
$ sudo /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' "list-nodes"

Id     Type     Status               Hostname  Pid   Tags  LastHeartbeatTime    ConfigVersion  ReleaseVersion
1      MGMTD    PRIMARY_MGMTD        v51       1471  []    N/A                  1(UPTODATE)    250228-dev-1-999999-3c100b90
100    META     HEARTBEAT_CONNECTED  v51       1469  []    2025-03-23 23:38:23  2(UPTODATE)    250228-dev-1-999999-3c100b90
10001  STORAGE  HEARTBEAT_CONNECTED  v52       1195  []    2025-03-23 23:38:26  5(UPTODATE)    250228-dev-1-999999-3c100b90
10002  STORAGE  HEARTBEAT_CONNECTED  v53       1573  []    2025-03-23 23:38:26  5(UPTODATE)    250228-dev-1-999999-3c100b90

```

---

## Step 7: Create admin user, storage targets and chain table

1. Create an admin user:
```sh
$ sudo /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' "user-add --root --admin 0 root"
Uid                0
Name               root
Token              AAB482S68QDJJijk2wCS+wV5(Expired at N/A)
IsRootUser         true
IsAdmin            true
Gid                0
SupplementaryGids  

$ sudo vi  /opt/3fs/etc/token.txt
$ sudo cat   /opt/3fs/etc/token.txt
AAB482S68QDJJijk2wCS+wV5
```
   The admin token is printed to the console, save it to `/opt/3fs/etc/token.txt`.

2. Generate `admin_cli` commands to create storage targets on 5 storage nodes (16 SSD per node, 6 targets per SSD).
* Follow instructions at [here](data_placement/README.md) to install Python packages.
```sh
$ sudo apt install python3-pip
$ pip install -r ~/3fs/deploy/data_placement/requirements.txt

$ python ~/3fs/deploy/data_placement/src/model/data_placement.py \
      -ql -relax -type CR --num_nodes 5 --replication_factor 3 --min_targets_per_disk 6

$ python ~/3fs/deploy/data_placement/src/setup/gen_chain_table.py \
      --chain_table_type CR --node_id_begin 10001 --node_id_end 10005 \
      --num_disks_per_node 16 --num_targets_per_disk 6 \
      --target_id_prefix 1 --chain_id_prefix 9 \
      --incidence_matrix_path output/DataPlacementModel-v_5-b_10-r_6-k_3-λ_2-lb_1-ub_1/incidence_matrix.pickle
```
==> The following 3 files will be generated in `output` directory: `create_target_cmd.txt`, `generated_chains.csv`, and `generated_chain_table.csv`.

```sh
$ sudo apt install python3-pip
$ pip3 install -r ~/3fs/deploy/data_placement/requirements.txt
$ python3 ~/3fs/deploy/data_placement/src/model/data_placement.py  -ql -relax -type CR --num_nodes 2 --replication_factor 2 --min_targets_per_disk 2

2025-03-23 23:17:57.867 | SUCCESS  | __main__:run:148 - saved solution to: output/DataPlacementModel-v_2-b_2-r_2-k_2-λ_2-lb_1-ub_0


$ python3 ~/3fs/deploy/data_placement/src/setup/gen_chain_table.py \
      --chain_table_type CR --node_id_begin 10001 --node_id_end 10002 \
      --num_disks_per_node 2 --num_targets_per_disk 2 \
      --target_id_prefix 1 --chain_id_prefix 9 \
      --incidence_matrix_path output/DataPlacementModel-v_2-b_2-r_2-k_2-λ_2-lb_1-ub_0/incidence_matrix.pickle

$ ls -l ~/3fs/output/
합계 24
drwxrwxr-x 2 jhyunlee jhyunlee 4096  3월 23 23:17 DataPlacementModel-v_2-b_2-r_2-k_2-λ_2-lb_1-ub_0
-rw-rw-r-- 1 jhyunlee jhyunlee 1142  3월 23 23:17 appsi_highs.log
-rw-rw-r-- 1 jhyunlee jhyunlee  920  3월 23 23:19 create_target_cmd.txt
-rw-rw-r-- 1 jhyunlee jhyunlee   48  3월 23 23:19 generated_chain_table.csv
-rw-rw-r-- 1 jhyunlee jhyunlee  170  3월 23 23:19 generated_chains.csv
-rw-rw-r-- 1 jhyunlee jhyunlee  888  3월 23 23:19 remove_target_cmd.txt      
```
==> The following 3 files will be generated in `output` directory: `create_target_cmd.txt`, `generated_chains.csv`, and `generated_chain_table.csv`.


3. Create storage targets:
```sh
$ sudo  /opt/3fs/bin/admin_cli --cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' --config.user_info.token $(<"/opt/3fs/etc/token.txt") < output/create_target_cmd.txt

Create target 101000100101 on disk 0 of 10001 succeeded
Create target 101000200101 on disk 0 of 10002 succeeded
Create target 101000100102 on disk 0 of 10001 succeeded
Create target 101000200102 on disk 0 of 10002 succeeded
Create target 101000100201 on disk 1 of 10001 succeeded
Create target 101000200201 on disk 1 of 10002 succeeded
Create target 101000100202 on disk 1 of 10001 succeeded
Create target 101000200202 on disk 1 of 10002 succeeded
```

4. Upload chains to mgmtd service:
```sh
$ sudo  /opt/3fs/bin/admin_cli --cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' --config.user_info.token $(<"/opt/3fs/etc/token.txt") "upload-chains output/generated_chains.csv"
Upload 4 chains succeeded
```

5. Upload chain table to mgmtd service:
```sh 
$ sudo   /opt/3fs/bin/admin_cli --cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' --config.user_info.token $(<"/opt/3fs/etc/token.txt") "upload-chain-table --desc stage 1 output/generated_chain_table.csv"
Upload ChainTableId(1) of ChainTableVersion(1) succeeded
```

6. List chains and chain tables to check if they have been correctly uploaded:
```sh
$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' "list-chains"
ChainId    ReferencedBy  ChainVersion  Status   PreferredOrder  Target                          Target
900100001  1             1             SERVING  []              101000100101(SERVING-UPTODATE)  101000200101(SERVING-ONLINE)
900100002  1             1             SERVING  []              101000100102(SERVING-UPTODATE)  101000200102(SERVING-ONLINE)
900200001  1             1             SERVING  []              101000100201(SERVING-UPTODATE)  101000200201(SERVING-ONLINE)
900200002  1             1             SERVING  []              101000100202(SERVING-UPTODATE)  101000200202(SERVING-ONLINE)

$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' "list-chain-tables"
ChainTableId  ChainTableVersion  ChainCount  ReplicaCount  Desc
1             1                  4           2             stage
```


---

## Step 8: FUSE client

For simplicity FUSE client is deployed on the **meta** node in this guide. However, we strongly advise against deploying clients on service nodes in production environment.

1. Copy `hf3fs_fuse_main` to `/opt/3fs/bin` and config files to `/opt/3fs/etc`.

```sh
$ sudo cp ~/3fs/build/bin/hf3fs_fuse_main /opt/3fs/bin
$ sudo cp ~/3fs/configs/{hf3fs_fuse_main_launcher.toml,hf3fs_fuse_main.toml,hf3fs_fuse_main_app.toml} /opt/3fs/etc
```
2. Create the mount point:
```sh
$ sudo mkdir -p /3fs/stage
```
3. Set cluster ID, mountpoint, token file and mgmtd address in [`hf3fs_fuse_main_launcher.toml`]
```sh
$ sudo vi /opt/3fs/etc/hf3fs_fuse_main_launcher.toml
```
```yaml
   cluster_id = "stage"
   mountpoint = '/3fs/stage'
   token_file = '/opt/3fs/etc/token.txt'

   [mgmtd_client]
   mgmtd_server_addresses = ["RDMA://192.168.1.1:8000"]
```

4. Set mgmtd and monitor address in [`hf3fs_fuse_main.toml`]
```sh
$ sudo vi  /opt/3fs/etc/hf3fs_fuse_main.toml
```

```yaml
   [mgmtd]
   mgmtd_server_addresses = ["RDMA://192.168.1.1:8000"]

   [common.monitor.reporters.monitor_collector]
   remote_ip = "192.168.1.1:10000"
```

5. Config file of FUSE client is also managed by mgmtd service. Use `admin_cli` to upload the config file to mgmtd:
```sh
$ sudo  /opt/3fs/bin/admin_cli \
        -cfg /opt/3fs/etc/admin_cli.toml \
        --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.51:8000"]' \
        "set-config --type FUSE --file /opt/3fs/etc/hf3fs_fuse_main.toml"

Succeed
ConfigVersion  1
```

6. Start FUSE client:

* Fuse 3.16 install 
```sh
$ wget --no-check-certificate https://github.com/libfuse/libfuse/releases/download/fuse-3.16.1/fuse-3.16.1.tar.gz
$ gunzip fuse-3.16.1.tar.gz
$ tar xvf  fuse-3.16.1.tar
$ sudo apt install -y meson ninja-build pkg-config libfuse3-dev
$ meson setup build
$ meson setup --wipe build   #<-- remove
$ meson setup --reconfigure  build  #<-- rebuild
$ cd build
$ ninja 
$ sudo ninja install 
$ fusermount3 --version 
$ lsmod  | grep fuse
$ sudo usermod -aG fuse $(whoami)
$ newgrp fuse
```

```sh
$ sudo cp ~/3fs/deploy/systemd/hf3fs_fuse_main.service /etc/systemd/system
$ sudo chmod 644 /etc/systemd/system/hf3fs_fuse_main.service
$ sudo cat  /etc/systemd/system/hf3fs_fuse_main.service
[Unit]
Description=fuse_main Server
Requires=network-online.target
After=network-online.target

[Service]
LimitNOFILE=1000000
ExecStart=/opt/3fs/bin/hf3fs_fuse_main --launcher_cfg /opt/3fs/etc/hf3fs_fuse_main_launcher.toml
Type=simple

[Install]
WantedBy=multi-user.target

$ sudo /opt/3fs/bin/hf3fs_fuse_main --launcher_cfg /opt/3fs/etc/hf3fs_fuse_main_launcher.toml
$ sudo systemctl daemon-reload
$ sudo systemctl start hf3fs_fuse_main
$ sudo systemctl enable hf3fs_fuse_main

```

7. Check if 3FS has been mounted at `/3fs/stage`:
```sh
$ sudo   mount | grep '/3fs/stage'

root@v51:/3fs/stage/t1# df
파일 시스템      1K-블록     사용      가용 사용% 마운트위치
hf3fs.stage    419225600  3063808 416161792    1% /3fs/stage
```

## FAQ

<details>
  <summary>How to troubleshoot <code>admin_cli init-cluster</code> error?</summary>

If mgmtd fails to start after running `init-cluster`, the most likely cause is an error in `mgmtd_main.toml`. Any changes to this file require clearing all FoundationDB data and re-running `init-cluster`

</details>

---

<details>
  <summary>How to build a single-node cluster?</summary>

A minimum of two storage services is required for data replication. If set `--num-nodes=1`, the `gen_chain_table.py` script will fail. In a test environment, this limitation can be bypassed by deploying multiple storage services on a single machine.

</details>

---

<details>
  <summary>How to update config files?</summary>

All config files are managed by mgmtd. If any `*_main.toml` is updated, such as `storage_main.toml`, the modified file should be uploaded using `admin_cli set-config`.

</details>

---

<details>
  <summary>How to troubleshoot common deployment issues?</summary>

When encountering any error during deployment,

- Check the log messages in `stdout/stderr` using `journalctl`, especially during service startup.
- Check log files stored in `/var/log/3fs/` on service and client nodes.
- Ensure that the directory `/var/log/3fs/` exists before starting any service.
</details>
