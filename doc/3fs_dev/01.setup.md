# 3FS dev
## 3FS build dependencies 
### time date 
* timedatectl
```sh
$ timedatectl status               
$ sudo timedatectl set-ntp true
$ sudo systemctl restart systemd-timesyncd
$ timedatectl status
```
* ntpdate 
```sh
$ sudo apt install ntpdate -y
$ sudo ntpdate time.google.com
```
### Install dependencies:
```bash
# for Ubuntu 22.04.
$ sudo  apt install cmake libuv1-dev liblz4-dev liblzma-dev libdouble-conversion-dev libdwarf-dev libunwind-dev \
  libaio-dev libgflags-dev libgoogle-glog-dev libgtest-dev libgmock-dev clang-format-14 clang-14 clang-tidy-14 lld-14 \
  libgoogle-perftools-dev google-perftools libssl-dev gcc-12 g++-12 libboost-all-dev

$ sudo apt install -y meson ninja-build pkg-config libfuse3-dev
$ sudo apt install curl wget 
$ sudo apt install nvme-cli xfsprogs
$ sudo apt install python3-pip
$ sudo apt install xfsprogs libboost-all-dev
$ sudo apt install libtcmalloc-minimal4 google-perftools libjemalloc2

```

### Rust 
#### sslVerify 
```sh
$ sudo apt install curl wget 
$ git config --global http.sslVerify false
$ git config --global --get http.sslVerify

$ cat ~/.wgetrc
check-certificate=off

$ cat ~/.curlrc
insecure

$ sudo apt update && sudo apt install --reinstall ca-certificates
```
#### Rust install 
```sh
$ curl --insecure --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

* 수동설치 
$ wget --no-check-certificate https://sh.rustup.rs -O rustup.init.w.sh
$ sh ./rustup-init.sh 

$ curl --insecure https://sh.rustup.rs  -sSf > rustup.init.c.sh
```
```sh
$ . "$HOME/.cargo/env"   
$ rustup update
$ rustup --version
$ cargo --version
$ rustc --version
```

#### Rust 수동 설치 
```sh
$ curl -L https://github.com/rust-lang/rustup/archive/refs/tags/1.26.0.tar.gz -o rustup.tar.gz
```
* 직접설치
=> 잘 안되면 https를 http로 수정해서 진행한다.
```sh
$ curl -O https://sh.rustup.rs
$ sh rustup-init.sh --no-modify-path
$ source $HOME/.cargo/env

$ export RUSTUP_DIST_SERVER=http://static.rust-lang.org
$ export RUSTUP_UPDATE_ROOT=http://static.rust-lang.org/rustup

$ rustup default stable
$ rustup update
$ rustup --version
$ cargo --version
$ rustc --version
```

### FoundationDB
* download & install  
```sh
$ wget --no-check-certificate  https://github.com/apple/foundationdb/releases/download/7.1.67/foundationdb-clients_7.1.67-1_amd64.deb
$ wget --no-check-certificate  https://github.com/apple/foundationdb/releases/download/7.1.67/foundationdb-server_7.1.67-1_amd64.deb
$ sudo dpkg -i foundationdb-server_7.1.67-1_amd64.deb
$ sudo dpkg -i foundationdb-clients_7.1.67-1_amd64.deb
```
###  Fuse 3.16
```sh
$ wget --no-check-certificate https://github.com/libfuse/libfuse/releases/download/fuse-3.16.1/fuse-3.16.1.tar.gz
$ gunzip fuse-3.16.1.tar.gz
$ tar xvf  fuse-3.16.1.tar
$ sudo apt install -y meson ninja-build pkg-config libfuse3-dev
$ meson setup build
$ meson setup --wipe build   #<-- remove
$ meson setup --reconfigure  build  #<-- rebuild
$ cd build
$ ninja 
$ sudo ninja install 
$ fusermount3 --version 
$ lsmod  | grep fuse
$ sudo usermod -aG fuse $(whoami)
$ newgrp fuse
```
* fuse github
```sh
$ sudo apt update
$ sudo apt install -y meson ninja-build pkg-config libfuse3-dev
$ git clone https://github.com/libfuse/libfuse.git
$ cd libfuse
$ meson setup build
$ cd build
$ ninja
$ sudo ninja install
$ fusermount3 --version
$ lsmod | grep fuse
$ sudo usermod -aG fuse $(whoami)
$ newgrp fuse
```

## Check out source code

### git repository 
```sh
git@github.com:jeonghyun7747/3FS.git
https://github.com/jeonghyun7747/3FS.git
```
###  Clone 3FS repository from GitHub:
```sh
$ git clone https://github.com/deepseek-ai/3fs
```
When `deepseek-ai/3fs` has been cloned to a local file system, run the
following commands to check out the submodules:

```bash
$ cd 3fs
$ git submodule update --init --recursive
$ ./patches/apply.sh
```

## Build 3FS

* Build 3FS in `build` folder:
```sh
$ cmake -S . -B build -DCMAKE_CXX_COMPILER=clang++-14 -DCMAKE_C_COMPILER=clang-14 -DCMAKE_BUILD_TYPE=RelWithDebInfo -DCMAKE_EXPORT_COMPILE_COMMANDS=ON
$ cmake --build build -j 6  
$ cmake --build build -j 6  -v 2>&1 | tee build3.log
```

### cmake 디버깅 모드 동작 
```sh
$ cmake -S . -B build -DCMAKE_CXX_COMPILER=clang++-14 -DCMAKE_C_COMPILER=clang-14 -DCMAKE_BUILD_TYPE=RelWithDebInfo -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DCMAKE_VERBOSE_MAKEFILE=ON
$ cmake --build build -j 16 -v 2>&1  | tee build_2.log

$ cmake --build build -j 16 -v -t hf3fs_common_shared
```
### 컴파일 결과 

```sh
jhyunlee@v70:~/3FS/build/bin$ ls -l
total 1548744
-rwxrwxr-x 1 jhyunlee jhyunlee 148480072 Jun  6 13:04 hf3fs-admin
-rwxrwxr-x 1 jhyunlee jhyunlee 209267656 Jun  6 13:11 hf3fs_fuse_main
-rwxrwxr-x 1 jhyunlee jhyunlee 284511104 Jun  6 13:11 meta_main
-rwxrwxr-x 1 jhyunlee jhyunlee 178916192 Jun  6 13:09 mgmtd_main
-rwxrwxr-x 1 jhyunlee jhyunlee 172360872 Jun  6 12:54 migration_main
-rwxrwxr-x 1 jhyunlee jhyunlee 105220160 Jun  6 12:35 monitor_collector_main
-rwxrwxr-x 1 jhyunlee jhyunlee 174745424 Jun  6 12:53 simple_example_main
-rwxrwxr-x 1 jhyunlee jhyunlee 312393136 Jun  6 13:03 storage_main
```


### Third-party dependencies 

#### ClickHouse

[https://clickhouse.com/docs/install](https://clickhouse.com/docs/install)

```sh
$ curl https://clickhouse.com/ | sh

Successfully downloaded the ClickHouse binary, you can run it as:
$ ./clickhouse

You can also install it:
$ sudo ./clickhouse install

Start clickhouse-server with:
$ sudo clickhouse start

Start clickhouse-client with:
$ clickhouse-client --password
v51 :) show users;
v51 :) show databases;
v51 :) use 3fs
v51 :) show tables;
```
- port 수정할 경우

```sh
$ sudo  vi  /etc/clickhouse-server/config.xml
$ <tcp_port>9500</tcp_port>

$ clickhouse-client --password --port 9500

<< 1234qwer
```

- reset

```sh
$ sudo  systemctl disable  clickhouse
Removed /etc/systemd/system/multi-user.target.wants/clickhouse.service.
$ sudo ./clickhouse stop
$ sudo rm /var/lib/clickhouse
$ sudo rm /var/log/clickhouse-server
$ sudo rm /etc/clickhouse-server/users.xml
$ sudo rm /etc/security/limits.d/clickhouse.conf
$ sudo rm /etc/clickhouse-server/users.xml
$ sudo rm /usr/lib/systemd/system/clickhouse.service
```
#####  systemd 설정

```sh
$ sudo vi /etc/systemd/system/clickhouse.service

[Unit]
Description=Clickhouse
After=network-online.target
Wants=network-online.target

[Service]
Type=oneshot
ExecStart=/usr/bin/clickhouse  start
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
```

```sh
$ sudo chmod  644 /usr/lib/systemd/system/clickhouse.service
$ sudo systemctl daemon-reload
$ sudo systemctl start clickhouse
$ sudo systemctl enable clickhouse
```


#### FoundationDB

- download & install
```sh
$ wget --no-check-certificate  https://github.com/apple/foundationdb/releases/download/7.1.67/foundationdb-clients_7.1.67-1_amd64.deb
$ wget --no-check-certificate  https://github.com/apple/foundationdb/releases/download/7.1.67/foundationdb-server_7.1.67-1_amd64.deb
$ sudo dpkg -i foundationdb-clients_7.1.67-1_amd64.deb  foundationdb-server_7.1.67-1_amd64.deb
```

* fdb.cluster, lib 
```sh
$ ll /etc/foundationdb//fdb.cluster 
-rw-rw-r-- 1 foundationdb foundationdb 33 Jun  6 10:25 /etc/foundationdb//fdb.cluster
$ cat /etc/foundationdb//fdb.cluster
9ZCAwhYS:CtoXYb5H@127.0.0.1:4500

$ ll /usr/lib/libfdb_c.so 
-rwxr-xr-x 1 root root 21027504 Jan 17 04:35 /usr/lib/libfdb_c.so*
```

## RoCE
```sh
$ sudo  apt  install rdma-core
$ sudo  apt  install rdma-core
$ sudo  apt install infiniband-diags
$ sudo  apt install perftest
$ sudo modprobe rdma-rxe
$ sudo rdma link add  rxe0 type rxe net enp0s9
$ sudo  ibv_devinfo
$ sudo ibstat 

$ sudo ib_write_bw -d rxe0 <<=== server 
$ sudo ib_write_bw 191.168.0.70 -d rxe0 <<==== client 
```
```sh
jhyunlee@v70:~/3FS/setup$ sudo ib_write_bw -d rxe0

************************************
* Waiting for client to connect... *
************************************
---------------------------------------------------------------------------------------
                    RDMA_Write BW Test
 Dual-port       : OFF		Device         : rxe0
 Number of qps   : 1		Transport type : IB
 Connection type : RC		Using SRQ      : OFF
 PCIe relax order: ON
 ibv_wr* API     : OFF
 CQ Moderation   : 1
 Mtu             : 1024[B]
 Link type       : Ethernet
 GID index       : 1
 Max inline data : 0[B]
 rdma_cm QPs	 : OFF
 Data ex. method : Ethernet
---------------------------------------------------------------------------------------
 local address: LID 0000 QPN 0x0012 PSN 0x597a3d RKey 0x000292 VAddr 0x0077690d55d000
 GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:00:70
 remote address: LID 0000 QPN 0x0011 PSN 0xa1849d RKey 0x000384 VAddr 0x0078fa6e2ad000
 GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:00:70
---------------------------------------------------------------------------------------
 #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]
 65536      5000             246.64             232.87 		   0.003726
---------------------------------------------------------------------------------------


jhyunlee@v70:~$ sudo ib_write_bw 192.168.0.70 -d rxe0
[sudo] password for jhyunlee: 
---------------------------------------------------------------------------------------
                    RDMA_Write BW Test
 Dual-port       : OFF		Device         : rxe0
 Number of qps   : 1		Transport type : IB
 Connection type : RC		Using SRQ      : OFF
 PCIe relax order: ON
 ibv_wr* API     : OFF
 TX depth        : 128
 CQ Moderation   : 1
 Mtu             : 1024[B]
 Link type       : Ethernet
 GID index       : 1
 Max inline data : 0[B]
 rdma_cm QPs	 : OFF
 Data ex. method : Ethernet
---------------------------------------------------------------------------------------
 local address: LID 0000 QPN 0x0011 PSN 0xa1849d RKey 0x000384 VAddr 0x0078fa6e2ad000
 GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:00:70
 remote address: LID 0000 QPN 0x0012 PSN 0x597a3d RKey 0x000292 VAddr 0x0077690d55d000
 GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:00:70
---------------------------------------------------------------------------------------
 #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]
 65536      5000             246.64             232.87 		   0.003726
---------------------------------------------------------------------------------------


$ rdma link show
link rxe0/1 state ACTIVE physical_state LINK_UP netdev enp0s9

```
##### rex service
```sh
$ sudo vi  /etc/systemd/system/rxe-setup.service

[Unit]
Description=RXE Setup 
After=network-online.target
Wants=network-online.target

[Service]
Type=oneshot
ExecStart=/usr/bin/rdma link add rxe0 type rxe net enp0s9 
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target

$ systemctl enable  rxe-setup
$ systemctl start  rxe-setup 
```

* Mellanox 드라이버  
```sh
$ pwd
$ sudo -i
root@v70:~# cd /
root@v70:/# tar xvf /home/jhyunlee/3FS/setup/ib.tar

$ /usr/sbin/ibdev2netdev
```

## Monitor Service 

### 1. clickhouser config

```sh
$ clickhouse-client --password --port 9500 < ~/3FS/deploy/sql/3fs-monitor.sql
Password for user (default):   1234qwer

$ clickhouse-client --password --port 9500
v21 :) show users;
v21 :) show databases;
v21 :) use 3fs
v21 :) show tables;

$ sudo ss -lntp
```

### 2. copy bin file 
```sh
$ sudo mkdir -p /opt/3fs/{bin,etc}
$ sudo mkdir -p /var/log/3fs
$ sudo cp ~/3FS/build/bin/monitor_collector_main /opt/3fs/bin
$ sudo cp ~/3FS/configs/monitor_collector_main.toml /opt/3fs/etc

$ sudo cp ~/3FS/build/bin/* /opt/3fs/bin
$ sudo cp ~/3FS/configs/* /opt/3fs/etc
```

### 3. config

```sh
$ sudo vi /opt/3fs/etc/monitor_collector_main.toml

   [server.monitor_collector.reporter]
   type = 'clickhouse'

   [server.monitor_collector.reporter.clickhouse]
   db = '3fs'
   host = '127.0.0.1'
   passwd = '1234qwer'
   port = '9500'
   user = 'default'
```

### 4.Start monitor service:

```sh
$  sudo cp  ~/3FS/deploy/systemd/monitor_collector_main.service /etc/systemd/system
$  sudo chmod  644 /etc/systemd/system/monitor_collector_main.service

[Unit]
Description=monitor_collector_main Server
Requires=network-online.target
After=network-online.target

[Service]
ExecStart=/opt/3fs/bin/monitor_collector_main --cfg /opt/3fs/etc/monitor_collector_main.toml
Type=simple

[Install]
WantedBy=multi-user.target

$ sudo systemctl daemon-reload
$ sudo systemctl start monitor_collector_main
$ sudo systemctl enable monitor_collector_main

$ sudo  systemctl restart monitor_collector_main
$ sudo  systemctl status  monitor_collector_main
```

## Admin 

### 1. config 
Install `admin_cli` on **all** nodes.

* Copy `admin_cli` to `/opt/3fs/bin` and config files to `/opt/3fs/etc`.

```sh
$ sudo cp  /etc/foundationdb/fdb.cluster  /opt/3fs/etc/fdb.cluster
```

* Update [`admin_cli.toml`] to set `cluster_id` and `clusterFile`:

```sh
$ sudo vi /opt/3fs/etc/admin_cli.toml
```

```yaml
cluster_id = "stage"

[fdb]
clusterFile = '/opt/3fs/etc/fdb.cluster'
```

The full help documentation for `admin_cli` can be displayed by running the following command:

```bash
$ sudo /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml help
```

## Mgmtd service

Install `mgmtd` service on **meta** node.
주의사항: 설치 할때 log를 확인하면서 진행한다

```sh
$ sudo tail  -f /var/log/3FS/mgmtd_main-err.log
$ sudo tail  -f /var/log/3FS/mgmtd_main.log
```

### 1. config 

```sh
$ sudo cp ~/3FS/build/bin/mgmtd_main /opt/3fs/bin
$ sudo cp ~/3FS/configs/{mgmtd_main.toml,mgmtd_main_launcher.toml,mgmtd_main_app.toml} /opt/3fs/etc
```

* Set mgmtd `node_id = 1`

```sh
$ sudo vi /opt/3fs/etc/mgmtd_main_app.toml
allow_empty_node_id = true
node_id = 1
```

* Edit [`mgmtd_main_launcher.toml`] to set the `cluster_id` and `clusterFile`:

```sh
$ sudo vi /opt/3fs/etc/mgmtd_main_launcher.toml

cluster_id = "stage"

[fdb]
clusterFile = '/opt/3fs/etc/fdb.cluster'
```

* Set monitor address in [`mgmtd_main.toml`]

```sh
$ sudo vi  /opt/3fs/etc/mgmtd_main.toml

   [common.monitor.reporters.monitor_collector]
   remote_ip = "192.168.0.70:10000"
```
### 2. Initialize the cluster:
* strip size 2로 설정 
```sh
$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml "init-cluster --mgmtd /opt/3fs/etc/mgmtd_main.toml 1 1048576 2"

Init filesystem, root directory layout: chain table ChainTableId(1), chunksize 1048576, stripesize 16
Init config for MGMTD version 1

$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml "init-cluster --allow-config-existed --mgmtd /opt/3fs/etc/mgmtd_main.toml 1 1048576 2"

```

The parameters of `admin_cli`:

> - `1` the chain table ID
> - `1048576` the chunk size in bytes
> - `2` the file strip size

Run `help init-cluster` for full documentation.

### 3. service

```bash
$  sudo cp ~/3FS/deploy/systemd/mgmtd_main.service /etc/systemd/system
$  sudo chmod  664 /etc/systemd/system/mgmtd_main.service
$  sudo cat /etc/systemd/system/mgmtd_main.service
[Unit]
Description=mgmtd_main Server
Requires=network-online.target
After=network-online.target

[Service]
LimitNOFILE=1000000
ExecStart=/opt/3fs/bin/mgmtd_main --launcher_cfg /opt/3fs/etc/mgmtd_main_launcher.toml --app-cfg /opt/3fs/etc/mgmtd_main_app.toml
Type=simple

[Install]
WantedBy=multi-user.target

$  sudo /opt/3fs/bin/mgmtd_main --launcher_cfg /opt/3fs/etc/mgmtd_main_launcher.toml --app-cfg /opt/3fs/etc/mgmtd_main_app.toml
$  sudo systemctl daemon-reload
$  sudo systemctl start mgmtd_main
$  sudo systemctl enable mgmtd_main

jhyunlee@v70:~/3FS/build/bin$ sudo  ss -nltp 
State    Recv-Q Send-Q  Local Address:Port    Address:Port  Process                                              
LISTEN   0      128         127.0.0.1:4500    0.0.0.0:*      users:(("fdbserver",pid=1018,fd=18))                
LISTEN   0      4096     192.168.0.70:8000    0.0.0.0:*      users:(("mgmtd_main",pid=32389,fd=45))              
LISTEN   0      128           0.0.0.0:22      0.0.0.0:*      users:(("sshd",pid=832,fd=3))                       
LISTEN   0      4096    127.0.0.53%lo:53      0.0.0.0:*      users:(("systemd-resolve",pid=558,fd=14))           
LISTEN   0      1024        127.0.0.1:39753   0.0.0.0:*      users:(("code-848b80aeb5",pid=2361,fd=9))           
LISTEN   0      4096        127.0.0.1:8123    0.0.0.0:*      users:(("clickhouse-serv",pid=12344,fd=64))         
LISTEN   0      128         127.0.0.1:631     0.0.0.0:*      users:(("cupsd",pid=809,fd=7))                      
LISTEN   0      4096        127.0.0.1:9004    0.0.0.0:*      users:(("clickhouse-serv",pid=12344,fd=67))         
LISTEN   0      4096        127.0.0.1:9005    0.0.0.0:*      users:(("clickhouse-serv",pid=12344,fd=38))         
LISTEN   0      4096        127.0.0.1:9009    0.0.0.0:*      users:(("clickhouse-serv",pid=12344,fd=34))         
LISTEN   0      4096        127.0.0.1:9500    0.0.0.0:*      users:(("clickhouse-serv",pid=12344,fd=66))         
LISTEN   0      4096     192.168.0.70:10000   0.0.0.0:*      users:(("monitor_collect",pid=13461,fd=29))         
LISTEN   0      4096     192.168.0.70:9000    0.0.0.0:*      users:(("mgmtd_main",pid=32389,fd=46))              
LISTEN   0      128              [::]:22         [::]:*      users:(("sshd",pid=832,fd=4))                       
LISTEN   0      128             [::1]:631        [::]:*      users:(("cupsd",pid=809,fd=6))                      
LISTEN   0      4096            [::1]:9009       [::]:*      users:(("clickhouse-serv",pid=12344,fd=33))         
LISTEN   0      4096            [::1]:9004       [::]:*      users:(("clickhouse-serv",pid=12344,fd=69))         
LISTEN   0      4096            [::1]:9005       [::]:*      users:(("clickhouse-serv",pid=12344,fd=61))         
LISTEN   0      4096            [::1]:9500       [::]:*      users:(("clickhouse-serv",pid=12344,fd=68))         
LISTEN   0      4096            [::1]:8123       [::]:*      users:(("clickhouse-serv",pid=12344,fd=65)) 
```

### 4. list-nodes

```sh
jhyunlee@v70:~/3FS/build/bin$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' "list-nodes"
Id  Type   Status         Hostname  Pid    Tags  LastHeartbeatTime  ConfigVersion  ReleaseVersion
1   MGMTD  PRIMARY_MGMTD  v70       32389  []    N/A                1(UPTODATE)    250228-dev-1-999999-32332570
```


## Meta Service
### 1. config 

```sh
$ sudo cp ~/3FS/build/bin/meta_main /opt/3fs/bin
$ sudo cp ~/3FS/configs/{meta_main_launcher.toml,meta_main.toml,meta_main_app.toml} /opt/3fs/etc
```
- Set meta `node_id = 100`

```sh
$ sudo vi   /opt/3fs/etc/meta_main_app.toml

allow_empty_node_id = true
node_id = 100
```

- Set `cluster_id`, `clusterFile` and mgmtd address in [`meta_main_launcher.toml`]

```sh
$ sudo vi  /opt/3fs/etc/meta_main_launcher.toml

cluster_id = "stage"

[mgmtd_client]
mgmtd_server_addresses = ["RDMA://192.168.0.70:8000"]
```

- Set mgmtd and monitor addresses in [`meta_main.toml`]

```sh
$ sudo vi  /opt/3fs/etc/meta_main.toml

   [server.mgmtd_client]
   mgmtd_server_addresses = ["RDMA://192.168.0.70:8000"]

   [common.monitor.reporters.monitor_collector]
   remote_ip = "192.168.0.70:10000"

   [server.fdb]
   clusterFile = '/opt/3fs/etc/fdb.cluster'
```

### 2. update config 
Config file of meta service is managed by mgmtd service. Use `admin_cli` to upload the config file to mgmtd:

- toml 파일의 설정이 변경될때 마다 다음 작업을 해줘서 버젼을 업데이트 해줘야 하다.

```sh
$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' "set-config --type META --file /opt/3fs/etc/meta_main.toml"

Succeed
ConfigVersion
```

### 3.Start meta service:

```bash
$ sudo  cp ~/3FS/deploy/systemd/meta_main.service /etc/systemd/system
$ sudo  chmod  644 /etc/systemd/system/meta_main.service
$ sudo  cat /etc/systemd/system/meta_main.service
[Unit]
Description=meta_main Server
Requires=network-online.target
After=network-online.target

[Service]
LimitNOFILE=1000000
ExecStart=/opt/3fs/bin/meta_main --launcher_cfg /opt/3fs/etc/meta_main_launcher.toml --app-cfg /opt/3fs/etc/meta_main_app.toml
Type=simple

[Install]
WantedBy=multi-user.target

$ sudo  /opt/3fs/bin/meta_main --launcher_cfg /opt/3fs/etc/meta_main_launcher.toml --app-cfg /opt/3fs/etc/meta_main_app.toml
$ sudo systemctl daemon-reload
$ sudo systemctl start meta_main
$ sudo systemctl enable meta_main

jhyunlee@v70:~/3FS/build/bin$ sudo  ss -nltp  | grep  main
[sudo] password for jhyunlee: 
LISTEN 0      4096    192.168.0.70:8001       0.0.0.0:*    users:(("meta_main",pid=36189,fd=53))      
LISTEN 0      4096    192.168.0.70:8000       0.0.0.0:*    users:(("mgmtd_main",pid=32389,fd=45))     
LISTEN 0      4096    192.168.0.70:9001       0.0.0.0:*    users:(("meta_main",pid=36189,fd=54))      
LISTEN 0      4096    192.168.0.70:9000       0.0.0.0:*    users:(("mgmtd_main",pid=32389,fd=46))
```

5. Run `list-nodes` command to check if meta service has joined the cluster:

```sh
$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' "list-nodes"
Id   Type   Status               Hostname  Pid    Tags  LastHeartbeatTime    ConfigVersion  ReleaseVersion
1    MGMTD  PRIMARY_MGMTD        v70       32389  []    N/A                  1(UPTODATE)    250228-dev-1-999999-32332570
100  META   HEARTBEAT_CONNECTED  v70       36189  []    2025-06-06 19:54:00  1(UPTODATE)    250228-dev-1-999999-32332570

```


##  Storage node 
### 1. disk setup 
* HDD 20GB nvme disk 4개
```sh
$ sudo lsblk 
$ sudo mkfs.xfs /dev/sdb
$ sudo mkfs.xfs /dev/sdc
$ sudo mkdir -p  /storage/data{1..2}
$ sudo mount -o noatime,nodiratime /dev/sdb /storage/data1
$ sudo mount -o noatime,nodiratime /dev/sdc /storage/data2
$ sudo mkdir -p /storage/data{1..2}/3fs
$ sudo mkdir -p /storage/data{1..2}/3fs
$ lsblk -o name,uuid,mountpoint
$ sudo  blkid /dev/sdb /dev/sdc
/dev/sdb: UUID="efa94240-2933-450c-b26f-9e64e3dd14fa" BLOCK_SIZE="512" TYPE="xfs"
/dev/sdc: UUID="b90401a5-eeef-4e18-a5da-2fe0353b26b8" BLOCK_SIZE="512" TYPE="xfs"

$ sudo vi /etc/fstab 
UUID=efa94240-2933-450c-b26f-9e64e3dd14fa /storage/data1 xfs defaults,noatime,nodiratime  0 2
UUID=b90401a5-eeef-4e18-a5da-2fe0353b26b8 /storage/data2 xfs defaults,noatime,nodiratime  0 2
```

* NVME 100GB * disk 2개

```sh
$ sudo apt install nvme-cli xfsprogs
$ sudo nvme list
$ sudo mkdir -p  /storage/data{1..4}
$ sudo -i
  for i in {1..4}; do mkfs.xfs -L data${i} /dev/nvme0n${i}; mount -o noatime,nodiratime /dev/nvme0n${i} /storage/data${i};done
  mkdir -p /storage/data{1..4}/3fs

$ sudo vi  /etc/fstab 
/dev/nvme0n1 /storage/data1 xfs noatime,nodiratime 
/dev/nvme0n2 /storage/data1 xfs noatime,nodiratime 
/dev/nvme0n3 /storage/data1 xfs noatime,nodiratime 
/dev/nvme0n4 /storage/data1 xfs noatime,nodiratime 

$ sudo  umount /storage/data{1..4}
$ sudo -i
  for i in {1..4}; do  mount  /storage/data${i}; done
```

### 2. os setup 

*  Increase the max number of asynchronous aio requests:
```sh
$ sudo sysctl -w fs.aio-max-nr=67108864
```

* nofile
```sh
$ sudo vi  /etc/security/limits.conf
* soft nofile 500000
* hard nofile 500000
root soft nofile 1000000
root hard nofile 1000000# <<----- sudo command에서 적용되는 값
```

* too many file open error
```sh
$ sudo vi /etc/sysctl.conf
fs.file-max = 2000000

$ sudo sysctl -p 
fs.file-max = 2000000
```


### 3.config  `storage_main` to `/opt/3fs/bin` and config files to `/opt/3fs/etc`.

```sh
$   ssh v52
$   sudo  scp  jhyunlee@meta:~/3fs/build/bin/storage_main /opt/3fs/bin
$   sudo  scp  jhyunlee@meta:~/3fs/configs/{storage_main_launcher.toml,storage_main.toml,storage_main_app.toml} /opt/3fs/etc
$   sudo  scp  jhyunlee@meta:~/3fs/configs/storage_main_app.toml /opt/3fs/etc
$   sudo  scp  jhyunlee@meta:~/3fs/configs/storage_main.toml /opt/3fs/etc
```

```sh
$ sudo vi /opt/3fs/etc/storage_main_app.toml
   allow_empty_node_id = true
   node_id = 10001
```

* Set `cluster_id` and mgmtd address in [`storage_main_launcher.toml`]

```sh
$  sudo vi /opt/3fs/etc/storage_main_launcher.toml

    cluster_id = "stage"

    [mgmtd_client]
    mgmtd_server_addresses = ["RDMA://192.168.0.70:8000"]
```

* Add target paths in [`storage_main.toml`]

```sh
$  sudo vi /opt/3fs/etc/storage_main.toml
   [server.mgmtd]
   mgmtd_server_addresses = ["RDMA://192.168.0.70:8000"]

   [common.monitor.reporters.monitor_collector]
   remote_ip = "192.168.0.70:10000"

   [server.targets]
   target_paths = ["/storage/data1/3fs","/storage/data2/3fs","/storage/data3/3fs","/storage/data4/3fs",]

   [server.base.groups.listener]
   listen_port = 8100   <<--------

   [server.base.groups.listener]
   listen_port = 9100   <<------
```

* reduce memory usage for singel node 
https://github.com/deepseek-ai/3FS/pull/180/files

### 4. tiny config 
```sh
$ sudo vi /opt/3fs/etc/storage_main.toml

  [server.aio_read_worker]
  enable_io_uring = false

  [server.buffer_pool]
  big_rdmabuf_count = 16
  big_rdmabuf_size = '16MB'
  rdmabuf_count = 32
  rdmabuf_size = '4MB'
  
  [server.targets.storage_target.kv_store]
  leveldb_block_cache_size = '1GB'
  rocksdb_block_cache_size = '1GB'
```
### 5. storage  등록 

 Config file of storage service is managed by mgmtd service. Use `admin_cli` to upload the config file to mgmtd:

```sh
$ sudo /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' "set-config --type STORAGE --file /opt/3fs/etc/storage_main.toml"

Succeed
ConfigVersion  1

```

### 6. Start storage service:

* toml 수정이 있은 다음에는 반드시 admin_cli -cfg를 통해서 설정 변경을 반영해야 한다. 
* admin_cli를 하지 않으면 전단계의 설정 파일 가지고 애를 쓰다가 잘 안된다. 
```sh
$ sudo scp  ~/3FS/deploy/systemd/storage_main.service /etc/systemd/system
$ sudo chmod 644  /etc/systemd/system/storage_main.service
$ sudo  cat   /etc/systemd/system/storage_main.service
[Unit]
Description=storage_main Server
Requires=network-online.target
After=network-online.target

[Service]
LimitNOFILE=1000000
LimitMEMLOCK=infinity
TimeoutStopSec=5m
ExecStart=/opt/3fs/bin/storage_main --launcher_cfg /opt/3fs/etc/storage_main_launcher.toml --app-cfg /opt/3fs/etc/storage_main_app.toml
Type=simple

[Install]
WantedBy=multi-user.target


$ sudo  /opt/3fs/bin/storage_main --launcher_cfg /opt/3fs/etc/storage_main_launcher.toml --app-cfg /opt/3fs/etc/storage_main_app.toml
$ sudo systemctl daemon-reload
$ sudo systemctl start storage_main
$ sudo systemctl enable storage_main
Created symlink /etc/systemd/system/multi-user.target.wants/storage_main.service → /etc/systemd/system/storage_main.service.

root@v52:~# ss -nltp
State           Recv-Q          Send-Q                    Local Address:Port                     Peer Address:Port          Process                                              
LISTEN          0               4096                       192.168.0.52:8000                          0.0.0.0:*              users:(("storage_main",pid=8138,fd=60))             
LISTEN          0               4096                       192.168.0.52:9000                          0.0.0.0:*              users:(("storage_main",pid=8138,fd=61))             
```


7. Run `list-nodes` command to check if storage service has joined the cluster:

```sh
$ sudo /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' "list-nodes"

Id     Type     Status               Hostname  Pid    Tags  LastHeartbeatTime    ConfigVersion  ReleaseVersion
1      MGMTD    PRIMARY_MGMTD        v70       878    []    N/A                  1(UPTODATE)    250228-dev-1-999999-32332570
100    META     HEARTBEAT_CONNECTED  v70       876    []    2025-06-06 20:46:38  1(UPTODATE)    250228-dev-1-999999-32332570
10001  STORAGE  HEARTBEAT_CONNECTED  v70       12240  []    2025-06-06 20:46:44  1(UPTODATE)    250228-dev-1-999999-32332570

```

## Create admin user, storage targets and chain table

### 1. Create an admin user:
```sh
$ sudo /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' "user-add --root --admin 0 root"
Uid                0
Name               root
Token              AADKWq5V8QBDbaHs2wBvjrZv(Expired at N/A)
IsRootUser         true
IsAdmin            true
Gid                0
SupplementaryGids

$ sudo cat > /opt/3fs/etc/token.txt <<EOF
AADKWq5V8QBDbaHs2wBvjrZv
EOF

$ sudo cat   /opt/3fs/etc/token.txt
AAB482S68QDJJijk2wCS+wV5
```

### 2. create storage targets

* Follow instructions at [here](data_placement/README.md) to install Python packages.
```sh
$ sudo apt install python3-pip
$ pip install -r ~/3FS/deploy/data_placement/requirements.txt

$ python3 ~/3FS/deploy/data_placement/src/model/data_placement.py \
      -ql -relax -type CR --num_nodes 5 --replication_factor 3 --min_targets_per_disk 6

$ python3 ~/3FS/deploy/data_placement/src/setup/gen_chain_table.py \
      --chain_table_type CR --node_id_begin 10001 --node_id_end 10005 \
      --num_disks_per_node 16 --num_targets_per_disk 6 \
      --target_id_prefix 1 --chain_id_prefix 9 \
      --incidence_matrix_path output/DataPlacementModel-v_5-b_10-r_6-k_3-λ_2-lb_1-ub_1/incidence_matrix.pickle
```
* 최소 2 Node와 2 replication 
```sh
$ python3 ~/3FS/deploy/data_placement/src/model/data_placement.py \
      -ql -relax -type CR --num_nodes 2 --replication_factor 2 --min_targets_per_disk 6

$ python3 ~/3FS/deploy/data_placement/src/setup/gen_chain_table.py \
      --chain_table_type CR --node_id_begin 10001 --node_id_end 10002 \
      --num_disks_per_node 4 --num_targets_per_disk 6 \
      --target_id_prefix 1 --chain_id_prefix 9 \
      --incidence_matrix_path output/DataPlacementModel-v_2-b_6-r_6-k_2-λ_6-lb_1-ub_0/incidence_matrix.pickle
```



==> The following 3 files will be generated in `output` directory: `create_target_cmd.txt`, `generated_chains.csv`, and `generated_chain_table.csv`.

```sh

jhyunlee@v70:~/3FS$  ls -l ~/3FS/output/
total 32
drwxrwxr-x 2 jhyunlee jhyunlee 4096 Jun  6 22:37 DataPlacementModel-v_2-b_6-r_6-k_2-λ_6-lb_1-ub_0
-rw-rw-r-- 1 jhyunlee jhyunlee 1142 Jun  6 22:37 appsi_highs.log
-rw-rw-r-- 1 jhyunlee jhyunlee 5520 Jun  6 22:38 create_target_cmd.txt
-rw-rw-r-- 1 jhyunlee jhyunlee  248 Jun  6 22:38 generated_chain_table.csv
-rw-rw-r-- 1 jhyunlee jhyunlee  890 Jun  6 22:38 generated_chains.csv
-rw-rw-r-- 1 jhyunlee jhyunlee 5328 Jun  6 22:38 remove_target_cmd.txt
     
```
==> The following 3 files will be generated in `output` directory: `create_target_cmd.txt`, `generated_chains.csv`, and `generated_chain_table.csv`.


### 3. Create storage targets:
```sh
$ sudo  /opt/3fs/bin/admin_cli --cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' --config.user_info.token $(<"/opt/3fs/etc/token.txt") < output/create_target_cmd.txt

Create target 101000100101 on disk 0 of 10001 succeeded
Create target 101000200101 on disk 0 of 10002 succeeded
Create target 101000100102 on disk 0 of 10001 succeeded
Create target 101000200102 on disk 0 of 10002 succeeded
Create target 101000100201 on disk 1 of 10001 succeeded
Create target 101000200201 on disk 1 of 10002 succeeded
Create target 101000100202 on disk 1 of 10001 succeeded
Create target 101000200202 on disk 1 of 10002 succeeded
```

### 4. Upload chains to mgmtd service:
```sh
$ sudo  /opt/3fs/bin/admin_cli --cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' --config.user_info.token $(<"/opt/3fs/etc/token.txt") "upload-chains output/generated_chains.csv"
Upload 4 chains succeeded
```

### 5. Upload chain table to mgmtd service:
```sh 
$ sudo   /opt/3fs/bin/admin_cli --cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' --config.user_info.token $(<"/opt/3fs/etc/token.txt") "upload-chain-table --desc stage 1 output/generated_chain_table.csv"
Upload ChainTableId(1) of ChainTableVersion(1) succeeded
```

### 6. List chains and chain tables to check if they have been correctly uploaded:
```sh
$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' "list-chains"
ChainId    ReferencedBy  ChainVersion  Status   PreferredOrder  Target                          Target
900100001  1             1             SERVING  []              101000100101(SERVING-UPTODATE)  101000200101(SERVING-ONLINE)
900100002  1             1             SERVING  []              101000100102(SERVING-UPTODATE)  101000200102(SERVING-ONLINE)
900200001  1             1             SERVING  []              101000100201(SERVING-UPTODATE)  101000200201(SERVING-ONLINE)
900200002  1             1             SERVING  []              101000100202(SERVING-UPTODATE)  101000200202(SERVING-ONLINE)

$ sudo  /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' "list-chain-tables"
ChainTableId  ChainTableVersion  ChainCount  ReplicaCount  Desc
1             1                  4           2             stage
```


## Step FUSE client

For simplicity FUSE client is deployed on the **meta** node in this guide. However, we strongly advise against deploying clients on service nodes in production environment.
### 1.file
* Copy `hf3fs_fuse_main` to `/opt/3fs/bin` and config files to `/opt/3fs/etc`.

```sh
$ sudo cp ~/3FS/build/bin/hf3fs_fuse_main /opt/3fs/bin
$ sudo cp ~/3FS/configs/{hf3fs_fuse_main_launcher.toml,hf3fs_fuse_main.toml,hf3fs_fuse_main_app.toml} /opt/3fs/etc
```
* Create the mount point:
```sh
$ sudo mkdir -p /3fs/stage
```
### 2. config 
* Set cluster ID, mountpoint, token file and mgmtd address in [`hf3fs_fuse_main_launcher.toml`]
```sh
$ sudo vi /opt/3fs/etc/hf3fs_fuse_main_launcher.toml

   cluster_id = "stage"
   mountpoint = '/3fs/stage'
   token_file = '/opt/3fs/etc/token.txt'

   [mgmtd_client]
   mgmtd_server_addresses = ["RDMA://192.168.0.70:8000"]
```

* Set mgmtd and monitor address in [`hf3fs_fuse_main.toml`]
```sh
$ sudo vi  /opt/3fs/etc/hf3fs_fuse_main.toml

   [mgmtd]
   mgmtd_server_addresses = ["RDMA://192.168.0.70:8000"]

   [common.monitor.reporters.monitor_collector]
   remote_ip = "192.168.0.70:10000"
```
### 3. client 등록
*. Config file of FUSE client is also managed by mgmtd service. Use `admin_cli` to upload the config file to mgmtd:
```sh
$ sudo  /opt/3fs/bin/admin_cli \
        -cfg /opt/3fs/etc/admin_cli.toml \
        --config.mgmtd_client.mgmtd_server_addresses '["RDMA://192.168.0.70:8000"]' \
        "set-config --type FUSE --file /opt/3fs/etc/hf3fs_fuse_main.toml"

Succeed
ConfigVersion  1
```
### 4. FUSE client:
```sh
$ sudo cp ~/3FS/deploy/systemd/hf3fs_fuse_main.service /etc/systemd/system
$ sudo chmod 644 /etc/systemd/system/hf3fs_fuse_main.service
$ sudo cat  /etc/systemd/system/hf3fs_fuse_main.service
[Unit]
Description=fuse_main Server
Requires=network-online.target
After=network-online.target

[Service]
LimitNOFILE=1000000
ExecStart=/opt/3fs/bin/hf3fs_fuse_main --launcher_cfg /opt/3fs/etc/hf3fs_fuse_main_launcher.toml
Type=simple

[Install]
WantedBy=multi-user.target

$ sudo /opt/3fs/bin/hf3fs_fuse_main --launcher_cfg /opt/3fs/etc/hf3fs_fuse_main_launcher.toml
$ sudo systemctl daemon-reload
$ sudo systemctl start hf3fs_fuse_main
$ sudo systemctl enable hf3fs_fuse_main

```
### 5. check  
Check if 3FS has been mounted at `/3fs/stage`:
```sh
$ sudo   mount | grep '/3fs/stage'

root@v51:/3fs/stage/t1# df
파일 시스템      1K-블록     사용      가용 사용% 마운트위치
hf3fs.stage    419225600  3063808 416161792    1% /3fs/stage
```
